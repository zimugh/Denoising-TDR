{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0344c601",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Name:-Denoising-TDR\" data-toc-modified-id=\"Name:-Denoising-TDR-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Name: Denoising-TDR</a></span></li><li><span><a href=\"#General-Information\" data-toc-modified-id=\"General-Information-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>General Information</a></span></li><li><span><a href=\"#Test-on-synthetic-images\" data-toc-modified-id=\"Test-on-synthetic-images-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Test on synthetic images</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Denoising\" data-toc-modified-id=\"Denoising-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Denoising</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gaussian-sigma=100\" data-toc-modified-id=\"Gaussian-sigma=100-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Gaussian sigma=100</a></span></li><li><span><a href=\"#Gaussian-sigma=224\" data-toc-modified-id=\"Gaussian-sigma=224-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Gaussian sigma=224</a></span></li><li><span><a href=\"#Poisson\" data-toc-modified-id=\"Poisson-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Poisson</a></span></li><li><span><a href=\"#Salt-and-pepper\" data-toc-modified-id=\"Salt-and-pepper-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Salt-and-pepper</a></span></li></ul></li></ul></li><li><span><a href=\"#HMI-LOS-magnetograms\" data-toc-modified-id=\"HMI-LOS-magnetograms-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>HMI LOS magnetograms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Denoising\" data-toc-modified-id=\"Denoising-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Denoising</a></span></li></ul></li><li><span><a href=\"#HST-images\" data-toc-modified-id=\"HST-images-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>HST images</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Denoising\" data-toc-modified-id=\"Denoising-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Denoising</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3bafc",
   "metadata": {},
   "source": [
    "## Name: Denoising-TDR\n",
    "\n",
    "In this repository we provide the code example of Denoising-TDR which is a universal denoising methods and can get denoised images meeting reasonable accuracy requirements.\n",
    "\n",
    "\n",
    "## General Information\n",
    "\n",
    "Please refer to (https://csyhquan.github.io/) to complete the basic software configuration and library installation. The following code is based on the repository of Self2Self (https://csyhquan.github.io/). \n",
    "\n",
    "For more information please see:\n",
    "- [[paper]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Quan_Self2Self_With_Dropout_Learning_Self-Supervised_Denoising_From_Single_Image_CVPR_2020_paper.pdf)\n",
    "- [[supmat]](http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Quan_Self2Self_With_Dropout_CVPR_2020_supplemental.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ecda9",
   "metadata": {},
   "source": [
    "## Test on synthetic images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbb66b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d747fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SAVE = 10000\n",
    "N_STEP = 100000\n",
    "N_SCALE = 100\n",
    "\n",
    "def train(file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    #gt = gt / N_SCALE\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "        for step in range(N_STEP):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                o_image = sum / N_PREDICTION #* N_SCALE\n",
    "                o_avg = o_avg #* N_SCALE\n",
    "                o_image = np.squeeze(o_image)\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                # sio.savemat(model_path + 'Self2Self-' + str(step + 1) + '.mat',\n",
    "                #             {'o_image': o_image, 'o_avg': o_avg})\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '.npy', o_image)\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '_slice_avg.npy', o_avg)\n",
    "                saver.save(sess, model_path + \"model.ckpt-\" + str(step + 1))\n",
    "\n",
    "            summary_writer.add_summary(merged, step)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = './data/trainingdata/'\n",
    "    file_list = os.listdir(path)\n",
    "    for file in file_list:\n",
    "        if '.npy' in file:\n",
    "            train(path + file, 0.3, -1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b552c",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738ea73",
   "metadata": {},
   "source": [
    "#### Gaussian sigma=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    #mid = np.median(gt)\n",
    "    #gt = gt - mid\n",
    "    #gt_copy = gt_copy - mid\n",
    "    #gt[gt_copy > 3000] = 3000\n",
    "    #gt[gt_copy < -3000] = -3000\n",
    "    #gt = gt / 100\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                #o_image = sum / N_PREDICTION * 100\n",
    "                #o_avg = o_avg * 100\n",
    "                #o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_avg[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_avg[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_image += mid\n",
    "                #o_avg += mid\n",
    "                o_image = np.squeeze(sum)/ N_PREDICTION\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                datao_image = np.array(o_image).astype(np.float32)\n",
    "                datao_avg = np.array(o_avg).astype(np.float32)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    path = './data/denoising_gaussian001/'\n",
    "    savepath='./results/data001gaussian/'\n",
    "    modelpath='./testsets/gauss1/data001gaussian23/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f8d4f",
   "metadata": {},
   "source": [
    "#### Gaussian sigma=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c192dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    #mid = np.median(gt)\n",
    "    #gt = gt - mid\n",
    "    #gt_copy = gt_copy - mid\n",
    "    #gt[gt_copy > 3000] = 3000\n",
    "    #gt[gt_copy < -3000] = -3000\n",
    "    #gt = gt / 100\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                #o_image = sum / N_PREDICTION * 100\n",
    "                #o_avg = o_avg * 100\n",
    "                #o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_avg[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_avg[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_image += mid\n",
    "                #o_avg += mid\n",
    "                o_image = np.squeeze(sum)/ N_PREDICTION\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                datao_image = np.array(o_image).astype(np.float32)\n",
    "                datao_avg = np.array(o_avg).astype(np.float32)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    pathmodel='/home/test/ltjupyter/work/1paperwork/20201015denoising/codes/ \\\n",
    "#    self2self-hubble10s/testsets/gauss/datagaussian23/-1/model/Self2Self/'\n",
    "    path = './data/denoising_gaussian005/'\n",
    "    savepath='./results/data005gaussian/'\n",
    "    modelpath='./testsets/gauss1/data001gaussian23/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20fac0",
   "metadata": {},
   "source": [
    "#### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SAVE = 10000\n",
    "\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    #mid = np.median(gt)\n",
    "    #gt = gt - mid\n",
    "    #gt_copy = gt_copy - mid\n",
    "    #gt[gt_copy > 3000] = 3000\n",
    "    #gt[gt_copy < -3000] = -3000\n",
    "    #gt = gt / 100\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                #o_image = sum / N_PREDICTION * 100\n",
    "                #o_avg = o_avg * 100\n",
    "                #o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_avg[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_avg[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_image += mid\n",
    "                #o_avg += mid\n",
    "                o_image = np.squeeze(sum)/ N_PREDICTION\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                datao_image = np.array(o_image).astype(np.float32)\n",
    "                datao_avg = np.array(o_avg).astype(np.float32)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    pathmodel='/home/test/ltjupyter/work/1paperwork/20201015denoising/codes/ \\\n",
    "#    self2self-hubble10s/testsets/gauss/datagaussian23/-1/model/Self2Self/'\n",
    "    path = './data/denoising_poisson_b1000b/'\n",
    "    savepath='./results/data001_poisson_b1000b/'\n",
    "    modelpath='./testsets/gauss1/data001gaussian23/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af624d",
   "metadata": {},
   "source": [
    "#### Salt-and-pepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    #mid = np.median(gt)\n",
    "    #gt = gt - mid\n",
    "    #gt_copy = gt_copy - mid\n",
    "    #gt[gt_copy > 3000] = 3000\n",
    "    #gt[gt_copy < -3000] = -3000\n",
    "    #gt = gt / 100\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                #o_image = sum / N_PREDICTION * 100\n",
    "                #o_avg = o_avg * 100\n",
    "                #o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_avg[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                #o_avg[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                #o_image += mid\n",
    "                #o_avg += mid\n",
    "                o_image = np.squeeze(sum)/ N_PREDICTION\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                datao_image = np.array(o_image).astype(np.float32)\n",
    "                datao_avg = np.array(o_avg).astype(np.float32)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    pathmodel='/home/test/ltjupyter/work/1paperwork/20201015denoising/codes/ \\\n",
    "#    self2self-hubble10s/testsets/gauss/datagaussian23/-1/model/Self2Self/'\n",
    "    path = './data/denoising_sp1all/'\n",
    "    savepath='./results/data001gt_d_datasp1/'\n",
    "    modelpath='./testsets/gauss1/data001gaussian23/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74592e",
   "metadata": {},
   "source": [
    "## HMI LOS magnetograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948157f2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5051969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SAVE = 10000\n",
    "N_STEP = 100000\n",
    "N_SCALE = 100\n",
    "\n",
    "def train(file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    gt = gt / N_SCALE\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "        for step in range(N_STEP):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                o_image = sum / N_PREDICTION * N_SCALE\n",
    "                o_avg = o_avg * N_SCALE\n",
    "                o_image = np.squeeze(o_image)\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                # sio.savemat(model_path + 'Self2Self-' + str(step + 1) + '.mat',\n",
    "                #             {'o_image': o_image, 'o_avg': o_avg})\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '.npy', o_image)\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '_slice_avg.npy', o_avg)\n",
    "                saver.save(sess, model_path + \"model.ckpt-\" + str(step + 1))\n",
    "\n",
    "            summary_writer.add_summary(merged, step)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = './data/trainingHMI/'\n",
    "    file_list = os.listdir(path)\n",
    "    for file in file_list:\n",
    "        if '.npy' in file:\n",
    "            train(path + file, 0.3, -1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4365e0b",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbeb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SCALE = 100\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    gt = gt / N_SCALE\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION* N_SCALE)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                o_image = np.squeeze(sum)/ N_PREDICTION* N_SCALE\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                datao_image = np.array(o_image).astype(np.float32)\n",
    "                datao_avg = np.array(o_avg).astype(np.float32)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    pathmodel='/home/test/ltjupyter/work/1paperwork/20201015denoising/codes/ \\\n",
    "#    self2self-hubble10s/testsets/gauss/datagaussian23/-1/model/Self2Self/'\n",
    "    path = './data/denoisingHMI/'\n",
    "    savepath='./results/hmilos0000/'\n",
    "    modelpath='./testsets/hmilos0000/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff784631",
   "metadata": {},
   "source": [
    "## HST images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da19d5e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SAVE = 10000\n",
    "N_STEP = 100000\n",
    "N_SCALE = 100\n",
    "\n",
    "def train(file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    mid = np.median(gt)\n",
    "    gt = gt - mid\n",
    "    gt_copy = gt_copy - mid\n",
    "    gt[gt_copy > 3000] = 3000\n",
    "    gt[gt_copy < -3000] = -3000\n",
    "    gt = gt / N_SCALE\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "        for step in range(N_STEP):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                o_image = sum / N_PREDICTION * N_SCALE\n",
    "                o_avg = o_avg * N_SCALE\n",
    "                o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                o_avg[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                o_avg[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                o_image += mid\n",
    "                o_avg += mid\n",
    "                o_image = np.squeeze(o_image)\n",
    "                o_avg = np.squeeze(o_avg)\n",
    "                # sio.savemat(model_path + 'Self2Self-' + str(step + 1) + '.mat',\n",
    "                #             {'o_image': o_image, 'o_avg': o_avg})\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '.npy', o_image)\n",
    "                np.save(model_path + 'Self2Self-' + str(step + 1) + '_slice_avg.npy', o_avg)\n",
    "                saver.save(sess, model_path + \"model.ckpt-\" + str(step + 1))\n",
    "\n",
    "            summary_writer.add_summary(merged, step)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = './data/trainingHST/'\n",
    "    file_list = os.listdir(path)\n",
    "    for file in file_list:\n",
    "        if '.npy' in file:\n",
    "            train(path + file, 0.3, -1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be50b6",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import network.Punet\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import util\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "TF_DATA_TYPE = tf.float32\n",
    "LEARNING_RATE = 1e-4\n",
    "N_PREDICTION = 100\n",
    "N_SCALE = 100\n",
    "\n",
    "\n",
    "def get_results(save_path, model_path, file_path, dropout_rate, sigma=25, is_realnoisy=False):\n",
    "    print(file_path)\n",
    "    tf.reset_default_graph()\n",
    "    gt = util.load_image_from_npy(file_path)\n",
    "    gt_copy = util.load_image_from_npy(file_path)\n",
    "    #### normalize\n",
    "    mid = np.median(gt)\n",
    "    gt = gt - mid\n",
    "    gt_copy = gt_copy - mid\n",
    "    gt[gt_copy > 3000] = 3000\n",
    "    gt[gt_copy < -3000] = -3000\n",
    "    gt = gt / N_SCALE\n",
    "\n",
    "    _, w, h, c = np.shape(gt)\n",
    "    #model_path = file_path[0:file_path.rfind(\".\")] + \"/\" + str(sigma) + \"/model/Self2Self/\"\n",
    "    #os.makedirs(model_path, exist_ok=True)\n",
    "    noisy = util.add_gaussian_noise(gt, model_path, sigma)\n",
    "    model = network.Punet.build_denoising_unet(noisy, 1 - dropout_rate, is_realnoisy)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    loss = model['training_error']\n",
    "    summay = model['summary']\n",
    "    saver = model['saver']\n",
    "    our_image = model['our_image']\n",
    "    is_flip_lr = model['is_flip_lr']\n",
    "    is_flip_ud = model['is_flip_ud']\n",
    "    avg_op = model['avg_op']\n",
    "    slice_avg = model['slice_avg']\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    avg_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "###############################################################################\n",
    "        nstep1=99999\n",
    "        saver.restore(sess, model_path+ \"model.ckpt-\" + str(nstep1 + 1))\n",
    "        sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "        \n",
    "        for j in range(N_PREDICTION):\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            # o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "            o_image = sess.run(our_image, feed_dict=feet_dict)\n",
    "            sum += o_image\n",
    "        #### renormalize\n",
    "        o_image = np.squeeze(sum / N_PREDICTION* N_SCALE)\n",
    "        name=list[i]\n",
    "        save_filename= '%s/%s' % (savepath, name)\n",
    "        sio.savemat(save_filename + '.mat', {'o_image': o_image})\n",
    "###############################################################################\n",
    "        retrain=10\n",
    "        N_SAVE=10\n",
    "        for step in range(retrain):\n",
    "###############################################################################\n",
    "            step=step+nstep1+1\n",
    "###############################################################################\n",
    "            feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "            _, _op, loss_value, merged, o_image = sess.run([optimizer, avg_op, loss, summay, our_image],\n",
    "                                                           feed_dict=feet_dict)\n",
    "            avg_loss += loss_value\n",
    "            if (step + 1) % N_SAVE == 0:\n",
    "\n",
    "                print(\"After %d training step(s)\" % (step + 1),\n",
    "                      \"loss  is {:.9f}\".format(avg_loss / N_SAVE))\n",
    "                avg_loss = 0\n",
    "                sum = np.float32(np.zeros(our_image.shape.as_list()))\n",
    "                for j in range(N_PREDICTION):\n",
    "                    feet_dict = {is_flip_lr: np.random.randint(0, 2), is_flip_ud: np.random.randint(0, 2)}\n",
    "                    o_avg, o_image = sess.run([slice_avg, our_image], feed_dict=feet_dict)\n",
    "                    sum += o_image\n",
    "                #     net_output = np.squeeze(np.uint8(np.clip(o_image, 0, 1) * 255))\n",
    "                #     cv2.imwrite(model_path + 'Self2Self-' + str(step + 1) + '_' + str(j + 1) + '.png', net_output)\n",
    "                # o_image = np.squeeze(np.uint8(np.clip(sum / N_PREDICTION, 0, 1) * 255))\n",
    "                # o_avg = np.squeeze(np.uint8(np.clip(o_avg, 0, 1) * 255))\n",
    "\n",
    "                #### de-normalize\n",
    "                o_image = sum / N_PREDICTION * N_SCALE\n",
    "                o_image[gt_copy > 3000] = gt_copy[gt_copy > 3000]\n",
    "                o_image[gt_copy < -3000] = gt_copy[gt_copy < -3000]\n",
    "                o_image += mid\n",
    "                o_image = np.squeeze(o_image)\n",
    "                \n",
    "                save_path = savepath+str(step+1)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                name=list[i]\n",
    "                save_filename= '%s/%s' % (save_path, name)\n",
    "                np.savez(save_filename, datao_image=datao_image,datao_avg=datao_avg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    pathmodel='/home/test/ltjupyter/work/1paperwork/20201015denoising/codes/ \\\n",
    "#    self2self-hubble10s/testsets/gauss/datagaussian23/-1/model/Self2Self/'\n",
    "    path = './data/denoisingHST/'\n",
    "    savepath='./results/npy_NGC1365_HST_F555W_10s_obs1.fits/'\n",
    "    modelpath='./testsets/npy_NGC1365_HST_F555W_10s_obs1.fits/-1/model/Self2Self/'\n",
    "    list = os.listdir(path)\n",
    "    list=sorted(list)\n",
    "    for i in range(len(list)):\n",
    "        get_results(savepath, modelpath, path + list[i], 0.3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698119a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcpu1",
   "language": "python",
   "name": "tfcpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
